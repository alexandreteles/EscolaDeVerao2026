{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4781a8354de433caca3e9eb4cd0c738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff1c5fd0f5f44271947c85ebe72125e4",
              "IPY_MODEL_3b7ba4055a674d0bb9b58b18fd300f26",
              "IPY_MODEL_702a3fee184149aa88e8b20aca8eb2b9"
            ],
            "layout": "IPY_MODEL_0a0e4aed515441c489772ad833b2b775"
          }
        },
        "ff1c5fd0f5f44271947c85ebe72125e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d7e1533f8c4a808dcdea362cb5dca4",
            "placeholder": "​",
            "style": "IPY_MODEL_74bf3b8e853f4d23b70547bdc00cd1ac",
            "value": "Loading weights: 100%"
          }
        },
        "3b7ba4055a674d0bb9b58b18fd300f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be56fde4418e4d828c07412fb6588b85",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0727674022ac47e4b2e8b61d20f75951",
            "value": 103
          }
        },
        "702a3fee184149aa88e8b20aca8eb2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd2b22d4e566434695a933aa74fe869e",
            "placeholder": "​",
            "style": "IPY_MODEL_9b0a3d05fda64c71b1dd1505a50c8666",
            "value": " 103/103 [00:00&lt;00:00, 493.09it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "0a0e4aed515441c489772ad833b2b775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d7e1533f8c4a808dcdea362cb5dca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74bf3b8e853f4d23b70547bdc00cd1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be56fde4418e4d828c07412fb6588b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0727674022ac47e4b2e8b61d20f75951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd2b22d4e566434695a933aa74fe869e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b0a3d05fda64c71b1dd1505a50c8666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gensim"
      ],
      "metadata": {
        "id": "-OBTEd7-8_p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "a4781a8354de433caca3e9eb4cd0c738",
            "ff1c5fd0f5f44271947c85ebe72125e4",
            "3b7ba4055a674d0bb9b58b18fd300f26",
            "702a3fee184149aa88e8b20aca8eb2b9",
            "0a0e4aed515441c489772ad833b2b775",
            "18d7e1533f8c4a808dcdea362cb5dca4",
            "74bf3b8e853f4d23b70547bdc00cd1ac",
            "be56fde4418e4d828c07412fb6588b85",
            "0727674022ac47e4b2e8b61d20f75951",
            "dd2b22d4e566434695a933aa74fe869e",
            "9b0a3d05fda64c71b1dd1505a50c8666"
          ]
        },
        "id": "uzAMtyiH6-6e",
        "outputId": "a8b3a3d1-fdb0-442b-833c-b76529fdb652"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4781a8354de433caca3e9eb4cd0c738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gensim.downloader as api\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Funções auxiliares\n",
        "# ----------------------------\n",
        "def mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    # Comentário: média dos embeddings dos tokens (ignorando padding) para virar embedding de sentença\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # [B, T, 1]\n",
        "    summed = (last_hidden_state * mask).sum(dim=1)                  # [B, H]\n",
        "    counts = mask.sum(dim=1).clamp(min=1e-9)                        # [B, 1]\n",
        "    return summed / counts\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_sentence(text: str) -> torch.Tensor:\n",
        "    # Comentário: gera embedding normalizado da sentença (bom para cosseno)\n",
        "    batch = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    out = model(**batch)\n",
        "    emb = mean_pooling(out.last_hidden_state, batch[\"attention_mask\"])\n",
        "    emb = F.normalize(emb, dim=1)\n",
        "    return emb[0].detach().cpu()  # [H]\n",
        "\n",
        "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:\n",
        "    # Comentário: cosseno entre dois vetores já normalizados\n",
        "    return float(torch.dot(a, b).item())\n",
        "\n",
        "def token_vector(word: str, embedding_weight: torch.Tensor) -> torch.Tensor:\n",
        "    # Comentário: pega o vetor do(s) token(s) que representam a palavra no vocabulário.\n",
        "    # Se a palavra vira vários sub-tokens, fazemos a média (só para fins didáticos).\n",
        "    toks = tokenizer.tokenize(word)\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    vec = embedding_weight[ids].mean(dim=0)\n",
        "    return F.normalize(vec, dim=0)\n",
        "\n",
        "def show_analogy(positive, negative, topn=10):\n",
        "    # Comentário: resolve a analogia: sum(positive) - sum(negative)\n",
        "    # e mostra os termos mais próximos pelo cosseno.\n",
        "    print(f\"\\nAnalogia: +{positive} -{negative} => ?\")\n",
        "    results = wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
        "    for word, score in results:\n",
        "        print(f\"  {word:<12} cosine={score:.4f}\")\n",
        "\n",
        "def show_direction_terms(base_word, target_word, topn=10):\n",
        "    # Comentário: mostra quais palavras estão na “direção” (target - base)\n",
        "    # Ex.: direção de France -> Italy tende a pegar países/atributos relacionados.\n",
        "    direction = wv[target_word] - wv[base_word]\n",
        "    results = wv.similar_by_vector(direction, topn=topn)\n",
        "    print(f\"\\nDireção: ({target_word} - {base_word}) -> termos próximos\")\n",
        "    for word, score in results:\n",
        "        print(f\"  {word:<12} cosine={score:.4f}\")"
      ],
      "metadata": {
        "id": "mMaB_5sb7ZvG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Embedding model e tokenizer: saída depende da tokenização\n",
        "# ============================================================\n",
        "text = \"Paris is the capital of France.\"\n",
        "encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0].tolist())\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(**encoded)\n",
        "    token_embs = out.last_hidden_state[0].detach().cpu()  # [T, H]\n",
        "\n",
        "print(\"=== (1) Tokenização e embeddings por token ===\")\n",
        "print(\"Texto:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Shape do output (last_hidden_state):\", tuple(out.last_hidden_state.shape), \"=> [batch, tokens, hidden]\")\n",
        "print(\"\\nPrimeiras dimensões do embedding de cada token (só para visualizar):\")\n",
        "for i, (tok, tok_id) in enumerate(zip(tokens, encoded[\"input_ids\"][0].tolist())):\n",
        "    preview = token_embs[i, :6].tolist()\n",
        "    print(f\"  {i:02d}  token={tok:<12} id={tok_id:<6} emb[:6]={[round(x, 4) for x in preview]}\")\n",
        "\n",
        "print(\"\\nExemplos rápidos de como a tokenização muda:\")\n",
        "examples = [\"unbelievable\", \"un-believable\", \"ChatGPT\", \"chat gpt\", \"São Paulo\", \"Sao Paulo\"]\n",
        "for ex in examples:\n",
        "    print(f\"  {ex!r} -> {tokenizer.tokenize(ex)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkY6bFg37fHl",
        "outputId": "2fd46a0f-28f9-4feb-a3e7-b1b55ca7e8b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== (1) Tokenização e embeddings por token ===\n",
            "Texto: Paris is the capital of France.\n",
            "Tokens: ['[CLS]', 'paris', 'is', 'the', 'capital', 'of', 'france', '.', '[SEP]']\n",
            "Shape do output (last_hidden_state): (1, 9, 384) => [batch, tokens, hidden]\n",
            "\n",
            "Primeiras dimensões do embedding de cada token (só para visualizar):\n",
            "  00  token=[CLS]        id=101    emb[:6]=[0.4117, 0.0816, 0.4398, -0.334, 0.4184, -0.1416]\n",
            "  01  token=paris        id=3000   emb[:6]=[0.5774, 0.6534, -0.1398, -0.5575, 0.3999, -0.1288]\n",
            "  02  token=is           id=2003   emb[:6]=[0.8619, 0.2646, 0.2367, -0.1776, 0.1201, -0.2658]\n",
            "  03  token=the          id=1996   emb[:6]=[0.2954, 0.0948, 0.2259, -0.1332, 0.3016, -0.2222]\n",
            "  04  token=capital      id=3007   emb[:6]=[1.3386, 0.2528, 0.0696, 0.0244, 0.501, -0.7151]\n",
            "  05  token=of           id=1997   emb[:6]=[0.3472, 0.1532, 0.0655, -0.1498, 0.1816, -0.3127]\n",
            "  06  token=france       id=2605   emb[:6]=[-0.0092, 0.1243, -0.0342, -0.5429, 0.6265, -0.1939]\n",
            "  07  token=.            id=1012   emb[:6]=[0.5565, 0.1231, 0.2209, -0.1343, 0.3973, -0.1652]\n",
            "  08  token=[SEP]        id=102    emb[:6]=[0.4826, 0.0459, 0.2783, -0.0069, 0.2975, -0.1594]\n",
            "\n",
            "Exemplos rápidos de como a tokenização muda:\n",
            "  'unbelievable' -> ['unbelievable']\n",
            "  'un-believable' -> ['un', '-', 'bel', '##ie', '##vable']\n",
            "  'ChatGPT' -> ['chat', '##gp', '##t']\n",
            "  'chat gpt' -> ['chat', 'gp', '##t']\n",
            "  'São Paulo' -> ['sao', 'paulo']\n",
            "  'Sao Paulo' -> ['sao', 'paulo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2) Similaridade cosseno entre sentenças\n",
        "# ============================================================\n",
        "print(\"\\n=== (2) Similaridade cosseno entre sentenças ===\")\n",
        "pairs = [\n",
        "    (\"A cat sits on the mat.\", \"A kitten is sitting on a mat.\"),\n",
        "    (\"I love pizza.\", \"The sky is blue.\"),\n",
        "    (\"He went to the bank to deposit money.\", \"He deposited cash at the bank.\"),\n",
        "]\n",
        "\n",
        "for s1, s2 in pairs:\n",
        "    e1 = embed_sentence(s1)\n",
        "    e2 = embed_sentence(s2)\n",
        "    sim = cosine_similarity(e1, e2)\n",
        "    print(f\"- s1: {s1}\")\n",
        "    print(f\"  s2: {s2}\")\n",
        "    print(f\"  cosine_similarity: {sim:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bElxWIa7i8l",
        "outputId": "8c55f2ad-7b67-4879-ce96-f946dd413886"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== (2) Similaridade cosseno entre sentenças ===\n",
            "- s1: A cat sits on the mat.\n",
            "  s2: A kitten is sitting on a mat.\n",
            "  cosine_similarity: 0.8582\n",
            "\n",
            "- s1: I love pizza.\n",
            "  s2: The sky is blue.\n",
            "  cosine_similarity: 0.0479\n",
            "\n",
            "- s1: He went to the bank to deposit money.\n",
            "  s2: He deposited cash at the bank.\n",
            "  cosine_similarity: 0.8920\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"glove-wiki-gigaword-50\"\n",
        "wv = api.load(model_name)\n",
        "\n",
        "print(\"Modelo carregado:\", model_name)\n",
        "print(\"Dimensão dos vetores:\", wv.vector_size)\n",
        "print(\"Tamanho do vocabulário:\", len(wv.key_to_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eupjl-Do7mJS",
        "outputId": "1806e4d8-106f-4598-c483-9eca259c0da3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Modelo carregado: glove-wiki-gigaword-50\n",
            "Dimensão dos vetores: 50\n",
            "Tamanho do vocabulário: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------\n",
        "# 1) Demonstração: analogias clássicas\n",
        "# -----------------------------------\n",
        "# Comentário: nem todas as analogias funcionam em todos os modelos,\n",
        "# mas em embeddings “clássicos” costuma aparecer bem.\n",
        "show_analogy(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=10)   # Paris - France + Italy\n",
        "show_analogy(positive=[\"rome\", \"france\"], negative=[\"italy\"], topn=10)    # Rome - Italy + France\n",
        "show_analogy(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10)       # king - man + woman ~ queen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXykjP9t8xVP",
        "outputId": "9f99e85a-48fa-4777-bd2b-8a5d4c7ddd4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analogia: +['paris', 'italy'] -['france'] => ?\n",
            "  rome         cosine=0.8466\n",
            "  milan        cosine=0.7766\n",
            "  turin        cosine=0.7666\n",
            "  venice       cosine=0.7592\n",
            "  madrid       cosine=0.7566\n",
            "  italian      cosine=0.7514\n",
            "  aires        cosine=0.7429\n",
            "  naples       cosine=0.7406\n",
            "  buenos       cosine=0.7357\n",
            "  lisbon       cosine=0.7245\n",
            "\n",
            "Analogia: +['rome', 'france'] -['italy'] => ?\n",
            "  paris        cosine=0.8582\n",
            "  prohertrib   cosine=0.7304\n",
            "  vienna       cosine=0.7211\n",
            "  french       cosine=0.7168\n",
            "  saint        cosine=0.7058\n",
            "  gaulle       cosine=0.6880\n",
            "  petersburg   cosine=0.6789\n",
            "  berlin       cosine=0.6758\n",
            "  geneva       cosine=0.6753\n",
            "  strasbourg   cosine=0.6597\n",
            "\n",
            "Analogia: +['king', 'woman'] -['man'] => ?\n",
            "  queen        cosine=0.8524\n",
            "  throne       cosine=0.7664\n",
            "  prince       cosine=0.7592\n",
            "  daughter     cosine=0.7474\n",
            "  elizabeth    cosine=0.7460\n",
            "  princess     cosine=0.7425\n",
            "  kingdom      cosine=0.7337\n",
            "  monarch      cosine=0.7214\n",
            "  eldest       cosine=0.7185\n",
            "  widow        cosine=0.7099\n"
          ]
        }
      ]
    }
  ]
}