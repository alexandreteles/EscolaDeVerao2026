{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47zT1nv7IoGM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Load TinyStories corpus\n",
        "# ---------------------------\n",
        "\n",
        "def load_tinystories(split=\"train\", max_examples=None):\n",
        "    ds = load_dataset(\"roneneldan/TinyStories\", split=split)  # [web:47][web:93]\n",
        "    texts = ds[\"text\"] if \"text\" in ds.column_names else ds[\"story\"]\n",
        "    if max_examples is not None:\n",
        "        texts = texts[:max_examples]\n",
        "    return texts"
      ],
      "metadata": {
        "id": "RmegkyNZK0Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 2. Train byte-level BPE tokenizer\n",
        "# ---------------------------\n",
        "\n",
        "def train_byte_bpe(texts, vocab_size, min_frequency=2, special_tokens=None):\n",
        "    if special_tokens is None:\n",
        "        special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
        "\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))  # [web:97][web:94]\n",
        "    tokenizer.pre_tokenizer = ByteLevel()          # byte-level pretokenizer\n",
        "    tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=min_frequency,\n",
        "        special_tokens=special_tokens,\n",
        "        show_progress=True,\n",
        "    )\n",
        "\n",
        "    # tokenizers expects an iterator over strings\n",
        "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "YM_Q-sEZLp1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3. Compute Zipf-style metric\n",
        "# ---------------------------\n",
        "\n",
        "def power_law(rank, a, b):\n",
        "    # log(freq) ≈ a + b * log(rank)  -> freq ≈ exp(a) * rank^b  [web:98][web:95]\n",
        "    return a + b * np.log(rank)\n",
        "\n",
        "def zipf_r2_from_counts(counts, top_k=None):\n",
        "    \"\"\"\n",
        "    counts: iterable of token counts (already sorted descending).\n",
        "    Returns R^2 of log(count) vs log(rank) fit.\n",
        "    \"\"\"\n",
        "    freqs = np.array(sorted(counts, reverse=True), dtype=np.float64)\n",
        "\n",
        "    if top_k is not None:\n",
        "        freqs = freqs[:top_k]\n",
        "\n",
        "    # ranks: 1..N\n",
        "    ranks = np.arange(1, len(freqs) + 1, dtype=np.float64)\n",
        "\n",
        "    # log-space\n",
        "    log_ranks = np.log(ranks)\n",
        "    log_freqs = np.log(freqs)\n",
        "\n",
        "    # fit log_freqs = a + b * log_ranks\n",
        "    popt, _ = curve_fit(power_law, ranks, log_freqs)  # [web:98][web:100]\n",
        "    a, b = popt\n",
        "\n",
        "    # predictions\n",
        "    log_freqs_pred = power_law(ranks, a, b)\n",
        "\n",
        "    # R^2\n",
        "    ss_res = np.sum((log_freqs - log_freqs_pred) ** 2)\n",
        "    ss_tot = np.sum((log_freqs - np.mean(log_freqs)) ** 2)\n",
        "    r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
        "\n",
        "    return r2, a, b"
      ],
      "metadata": {
        "id": "3DI-XFS0Lq4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 4. Evaluate a tokenizer on Zipf-style metric\n",
        "# ---------------------------\n",
        "\n",
        "def tokenizer_zipf_score(tokenizer, texts, max_docs=None, top_k=None):\n",
        "    \"\"\"\n",
        "    Tokenize a subset of texts, compute token frequency,\n",
        "    then fit Zipf-like curve and return R^2 and basic stats.\n",
        "    \"\"\"\n",
        "    if max_docs is not None:\n",
        "        texts = texts[:max_docs]\n",
        "\n",
        "    counter = Counter()\n",
        "\n",
        "    for t in texts:\n",
        "        ids = tokenizer.encode(t).ids\n",
        "        counter.update(ids)\n",
        "\n",
        "    counts = list(counter.values())\n",
        "    total_tokens = sum(counts)\n",
        "    vocab_in_use = len(counts)\n",
        "\n",
        "    r2, a, b = zipf_r2_from_counts(counts, top_k=top_k)\n",
        "\n",
        "    stats = {\n",
        "        \"r2\": r2,\n",
        "        \"a\": a,\n",
        "        \"b\": b,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"vocab_in_use\": vocab_in_use,\n",
        "    }\n",
        "    return stats"
      ],
      "metadata": {
        "id": "8vlEL7LeL03P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_sizes = [4096, 8192, 16384]\n",
        "max_train_examples = 20000       # for tokenizer training\n",
        "max_eval_docs = 5000             # for Zipf metric evaluation\n",
        "top_k_zipf = 5000                # fit Zipf on top-K most frequent tokens\n",
        "\n",
        "print(\"Loading TinyStories texts...\")\n",
        "texts = load_tinystories(split=\"train\", max_examples=max_train_examples)\n",
        "print(f\"Loaded {len(texts)} examples.\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for vs in vocab_sizes:\n",
        "    print(f\"\\n=== Training tokenizer with vocab_size={vs} ===\")\n",
        "    tok = train_byte_bpe(texts, vocab_size=vs)\n",
        "\n",
        "    print(\"Computing Zipf-style statistics...\")\n",
        "    stats = tokenizer_zipf_score(tok, texts, max_docs=max_eval_docs, top_k=top_k_zipf)\n",
        "\n",
        "    print(f\"Vocab size (target): {vs}\")\n",
        "    print(f\"Vocab size (in use): {stats['vocab_in_use']}\")\n",
        "    print(f\"Total tokens (sample): {stats['total_tokens']}\")\n",
        "    print(f\"Zipf fit R^2: {stats['r2']:.4f}  (a={stats['a']:.3f}, b={stats['b']:.3f})\")\n",
        "\n",
        "    results.append((vs, stats))\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "for vs, stats in results:\n",
        "    print(\n",
        "        f\"vocab={vs:5d} | in_use={stats['vocab_in_use']:5d} | \"\n",
        "        f\"tokens={stats['total_tokens']:8d} | R^2={stats['r2']:.4f}\"\n",
        "    )\n",
        "\n",
        "# Pick the vocab size with the highest R^2\n",
        "best_vs, best_stats = max(results, key=lambda x: x[1][\"r2\"])\n",
        "print(f\"\\nBest by Zipf R^2: vocab_size={best_vs}, R^2={best_stats['r2']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB1DrD9cMC-l",
        "outputId": "d834a8ab-31e3-4a1d-c2c3-896e147d4735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyStories texts...\n",
            "Loaded 20000 examples.\n",
            "\n",
            "=== Training tokenizer with vocab_size=4096 ===\n",
            "Computing Zipf-style statistics...\n",
            "Vocab size (target): 4096\n",
            "Vocab size (in use): 3880\n",
            "Total tokens (sample): 1052178\n",
            "Zipf fit R^2: 0.8368  (a=14.309, b=-1.425)\n",
            "\n",
            "=== Training tokenizer with vocab_size=8192 ===\n",
            "Computing Zipf-style statistics...\n",
            "Vocab size (target): 8192\n",
            "Vocab size (in use): 7132\n",
            "Total tokens (sample): 1010099\n",
            "Zipf fit R^2: 0.9823  (a=13.970, b=-1.397)\n",
            "\n",
            "=== Training tokenizer with vocab_size=16384 ===\n",
            "Computing Zipf-style statistics...\n",
            "Vocab size (target): 16384\n",
            "Vocab size (in use): 9485\n",
            "Total tokens (sample): 1001119\n",
            "Zipf fit R^2: 0.9725  (a=14.448, b=-1.478)\n",
            "\n",
            "=== Summary ===\n",
            "vocab= 4096 | in_use= 3880 | tokens= 1052178 | R^2=0.8368\n",
            "vocab= 8192 | in_use= 7132 | tokens= 1010099 | R^2=0.9823\n",
            "vocab=16384 | in_use= 9485 | tokens= 1001119 | R^2=0.9725\n",
            "\n",
            "Best by Zipf R^2: vocab_size=8192, R^2=0.9823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a38741f",
        "outputId": "7bb572d0-69b8-40d1-fbd4-7ea3032401a1"
      },
      "source": [
        "import random\n",
        "\n",
        "# Re-train the best tokenizer using best_vs\n",
        "print(f\"\\n=== Re-training the best tokenizer with vocab_size={best_vs} ===\")\n",
        "best_tokenizer = train_byte_bpe(texts, vocab_size=best_vs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Re-training the best tokenizer with vocab_size=8192 ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select random stories from the dataset\n",
        "random_stories = random.sample(texts, 1)\n",
        "\n",
        "print(\"\\n=== Encoding and Decoding Random Stories ===\")\n",
        "for i, story in enumerate(random_stories):\n",
        "    print(f\"\\n--- Story {i+1} ---\")\n",
        "    print(\"Original:\")\n",
        "    print(story)\n",
        "\n",
        "    # Encode the story\n",
        "    encoded_tokens = best_tokenizer.encode(story)\n",
        "    print(\"\\nEncoded IDs (first 20):\", encoded_tokens.ids[:20])\n",
        "\n",
        "    # Decode the story\n",
        "    decoded_story = best_tokenizer.decode(encoded_tokens.ids)\n",
        "    print(\"\\nDecoded:\")\n",
        "    print(decoded_story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t58dp_mXMuEM",
        "outputId": "d6edcc4d-2b98-42c1-f26f-37db16844138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Encoding and Decoding Random Stories ===\n",
            "\n",
            "--- Story 1 ---\n",
            "Original:\n",
            "Lily wanted to teach her doll how to drink wine. She saw her mom and dad drink wine sometimes, and they said it was good. Lily found a bottle of wine in the kitchen and poured some in a cup. She brought the cup and the doll to her room.\n",
            "\n",
            "\"Look, doll, this is wine. It is a drink for grown-ups. You can try some, but only a little bit, okay?\" Lily said to her doll. She held the cup to the doll's mouth and pretended to make her drink. \"Mmm, do you like it? It is sweet and sour.\"\n",
            "\n",
            "But then, Lily heard a knock on the door. It was her mom. She opened the door and saw Lily and the doll with the cup of wine. She was very angry and surprised.\n",
            "\n",
            "\"Lily, what are you doing? Where did you get that wine? You know you are not allowed to touch that!\" her mom said in a loud voice.\n",
            "\n",
            "Lily felt embarrassed and scared. She dropped the cup and the wine spilled on the floor. She started to cry and hugged her doll.\n",
            "\n",
            "\"I'm sorry, mom. I just wanted to teach my doll how to drink wine. I didn't know it was bad,\" Lily said between sobs.\n",
            "\n",
            "Her mom sighed and picked up Lily and the doll. She took them to the bathroom and cleaned them up. She also cleaned the floor and threw away the cup.\n",
            "\n",
            "\"Lily, wine is not for children. It can make you sick and hurt your body. You should never drink it or play with it. Do you understand?\" her mom said in a calm voice.\n",
            "\n",
            "Lily nodded and said sorry again. She felt sorry for making a mess and making her mom angry. She also felt sorry for her doll, who didn't like wine either.\n",
            "\n",
            "Her mom hugged her and said she loved her. She said she was not angry anymore, but she wanted Lily to be careful and listen to her. She said she would teach her about wine when she was older and ready.\n",
            "\n",
            "Lily said she loved her mom too. She said she would be careful and listen to her. She said she would teach her doll something else, like how to sing or dance.\n",
            "\n",
            "Her mom smiled and kissed her. She said she was proud of her for being honest and brave. She said she would like to see her doll sing or dance.\n",
            "\n",
            "Lily smiled and kissed her back. She felt happy and safe. She took her doll and went to her room. She forgot about the wine and taught her doll how to sing a song.\n",
            "\n",
            "Encoded IDs (first 20): [206, 293, 115, 2058, 158, 770, 558, 115, 1856, 4663, 15, 162, 261, 158, 212, 114, 482, 1856, 4663, 1430]\n",
            "\n",
            "Decoded:\n",
            " Lily wanted to teach her doll how to drink wine. She saw her mom and dad drink wine sometimes, and they said it was good. Lily found a bottle of wine in the kitchen and poured some in a cup. She brought the cup and the doll to her room.\n",
            "\n",
            "\"Look, doll, this is wine. It is a drink for grown-ups. You can try some, but only a little bit, okay?\" Lily said to her doll. She held the cup to the doll's mouth and pretended to make her drink. \"Mmm, do you like it? It is sweet and sour.\"\n",
            "\n",
            "But then, Lily heard a knock on the door. It was her mom. She opened the door and saw Lily and the doll with the cup of wine. She was very angry and surprised.\n",
            "\n",
            "\"Lily, what are you doing? Where did you get that wine? You know you are not allowed to touch that!\" her mom said in a loud voice.\n",
            "\n",
            "Lily felt embarrassed and scared. She dropped the cup and the wine spilled on the floor. She started to cry and hugged her doll.\n",
            "\n",
            "\"I'm sorry, mom. I just wanted to teach my doll how to drink wine. I didn't know it was bad,\" Lily said between sobs.\n",
            "\n",
            "Her mom sighed and picked up Lily and the doll. She took them to the bathroom and cleaned them up. She also cleaned the floor and threw away the cup.\n",
            "\n",
            "\"Lily, wine is not for children. It can make you sick and hurt your body. You should never drink it or play with it. Do you understand?\" her mom said in a calm voice.\n",
            "\n",
            "Lily nodded and said sorry again. She felt sorry for making a mess and making her mom angry. She also felt sorry for her doll, who didn't like wine either.\n",
            "\n",
            "Her mom hugged her and said she loved her. She said she was not angry anymore, but she wanted Lily to be careful and listen to her. She said she would teach her about wine when she was older and ready.\n",
            "\n",
            "Lily said she loved her mom too. She said she would be careful and listen to her. She said she would teach her doll something else, like how to sing or dance.\n",
            "\n",
            "Her mom smiled and kissed her. She said she was proud of her for being honest and brave. She said she would like to see her doll sing or dance.\n",
            "\n",
            "Lily smiled and kissed her back. She felt happy and safe. She took her doll and went to her room. She forgot about the wine and taught her doll how to sing a song.\n",
            "\n",
            "(Original and Decoded differ)\n"
          ]
        }
      ]
    }
  ]
}